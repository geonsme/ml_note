假设损失函数是$J(W,x,y)$, 有了样本$\{(x_1,y_1), ... ,(x_m, y_m)\}$后，我们需要找出W使得  
$$L(W)=\sum_{i=0}^mJ(W,x_i,y_i)$$  
最小。  
{说一种特殊情况，
假设$J(W,x_i, y_i)=(Wx_i-y_i)^2$,也就是说要求  
$$W^*=\arg\min_W\sum_{i=0}^m(Wx_i-y_i)^2$$  
显然最好的结果是每个式子都等于0。在这种情况下，假设$W=(w_1,..,w_m)$，且各个等式是线性无关的，那么很容易解出$W=X^{-1}y$, 其中$X=(x_1,...,x_2), x_i=(x_i^1,...,x_i^m)$  
那么当等式数量小于未知数数量时(样本不足，或者某种意义上可以理解为约束不足，导致可以有多个解，解的自由度很高所以可以照顾到每一个样本)，就会发生过拟合。反之，等式数量多于未知数数量，则容易发生欠拟合。  
"最好的结果是每个式子都等于0"这句话其实不妥。如果L是凸函数, 那么W是最优解的充要条件是L在W处的梯度为0，即$\nabla L(W)=0$。那为了能够直接求解出W，还需要满足上式有解析解(解析就是给出解的具体函数形式，从解的表达式中就可以算出任何对应值。不一定存在解析解，比如5次方程；如果没有解析解，就只能求数值解，机器学习中基本都是数值解。)  
}