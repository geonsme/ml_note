假设损失函数是$J(W,x,y)$, 有了样本$\{(x_1,y_1), ... ,(x_m, y_m)\}$后，我们需要找出W使得  
$$L(W)=\sum_{i=0}^mJ(W,x_i,y_i)$$  
最小。  
{说一种特殊情况，
假设$J(W,x_i, y_i)=(Wx_i-y_i)^2$,也就是说要求  
$$W^*=\arg\min_W\sum_{i=0}^m(Wx_i-y_i)^2$$  
显然最好的结果是每个式子都等于0。在这种情况下，假设$W=(w_1,..,w_m)$，且各个等式是线性无关的，那么很容易解出$W=X^{-1}y$, 其中$X=(x_1,...,x_2), x_i=(x_i^1,...,x_i^m)$  
那么当等式数量小于未知数数量时(样本不足，或者某种意义上可以理解为约束不足，导致可以有多个解，解的自由度很高所以可以照顾到每一个样本)，就会发生过拟合。反之，等式数量多于未知数数量，则容易发生欠拟合。  
"最好的结果是每个式子都等于0"这句话其实不妥。如果L是凸函数, 那么W是最优解的充要条件是L在W处的梯度为0，即$\nabla L(W)=0$。那为了能够直接求解出W，还需要满足上式有解析解(解析就是给出解的具体函数形式，从解的表达式中就可以算出任何对应值。不一定存在解析解，比如5次方程；如果没有解析解，就只能求数值解，机器学习中基本都是数值解。)  
}  

注意到L(w)用到了所有的样本，但实际过程中因为数据是海量的，所以直接优化L(W)是不切实际的。因此提出随机梯度下降法:每次从所有样本中随机抽出n个样本进行梯度下降, n << m 。参数更新：  
$$\nabla L(W)=\frac{1}{N}\sum_{i=1}^{n}\nabla L(W, x_i, y_i)$$  
$$W_{i+1}=W_{i}-\alpha \nabla L(W_i)$$  
这里有三点值得注意：  
* 怎么选择参数n？一般设为2的幂次以充分利用矩阵运算。
* 如何选择n个训练数据？每次遍历训练数据前对所有数据进行随机排序，然后每次迭代时按顺序挑选n个训练数据直到遍历完所有的数据，这样可以避免数据的特定顺序对算法收敛带来影响。
* 如何选择学习速率α？通常采用衰减学习速率的方案：一开始算法采用较大的学习率，当误差曲线进入平台期后，减小学习速率做更精细调整。  

-------

随机梯度下降法存在的缺点：
---
1. 陷入局部最优解。
2. 鞍点。

为了解决这两个缺点，有以下几种方法:  
1. 动量方法：修改每次前进的步伐, 让前一次步伐$v_{t-1}$参与对本次步伐的更新：  
$$v_t=\gamma v_{t-1} + \eta g_t$$  
$$\theta_{t+1}=\theta_t - v_t$$  
当设$v_0=0$时，有$v_t=\frac{1-r^t}{1-r}\eta g_t$, $\gamma$一般取0.9。

2. AdaGrad: 具体到每个参数的更新上，不同参数的更新步幅是不同的，比如对于样本$x_i=(x_i^1,...,x_i^m)$ , 如果$x_i^1$很少不为0，那么对应的$w_1$更新频率也就很低。而我们希望不同参数有自适应的学习速率，也就是更新频率低的参数有较大的更新步幅，反之则步幅减小。AdaGrad采用“历史梯度平方和”来衡量不同参数的梯度的稀疏性，具体更新公式为:  
$$\theta_{t+1,j}=\theta_{t,j}-\frac{\eta}{\sqrt{\sum_{k=0}^t g_{k,j}^2 + \epsilon}}g_{t,j}$$  
分母中实现了退火过程，也就是随着时间推移，学习速率越来越小，保证算法最终收敛。