假设损失函数是$J(W,x,y)$, 有了样本$\{(x_1,y_1), ... ,(x_m, y_m)\}$后，我们需要找出W使得  
$$L(W)=\sum_{i=0}^mJ(W,x_i,y_i)$$  
最小。  
{说一种特殊情况，
假设$J(W,x_i, y_i)=(Wx_i-y_i)^2$,也就是说要求  
$$W^*=\arg\min_W\sum_{i=0}^m(Wx_i-y_i)^2$$  
显然最好的结果是每个式子都等于0。在这种情况下，假设$W=(w_1,..,w_m)$，且各个等式是线性无关的，那么很容易解出$W=X^{-1}y$, 其中$X=(x_1,...,x_2), x_i=(x_i^1,...,x_i^m)$  
那么当等式数量小于未知数数量时(样本不足，或者某种意义上可以理解为约束不足，导致可以有多个解，解的自由度很高所以可以照顾到每一个样本)，就会发生过拟合。反之，等式数量多于未知数数量，则容易发生欠拟合。  
"最好的结果是每个式子都等于0"这句话其实不妥。如果L是凸函数, 那么W是最优解的充要条件是L在W处的梯度为0，即$\nabla L(W)=0$。那为了能够直接求解出W，还需要满足上式有解析解(解析就是给出解的具体函数形式，从解的表达式中就可以算出任何对应值。不一定存在解析解，比如5次方程；如果没有解析解，就只能求数值解，机器学习中基本都是数值解。)  
}  

注意到L(w)用到了所有的样本，但实际过程中因为数据是海量的，所以直接优化L(W)是不切实际的。因此提出随机梯度下降法:每次从所有样本中随机抽出n个样本进行梯度下降, n << m 。参数更新：  
$$\nabla L(W)=\frac{1}{N}\sum_{i=1}^{n}\nabla L(W, x_i, y_i)$$  
$$W_{i+1}=W_{i}-\alpha \nabla L(W_i)$$  
这里有三点值得注意：  
* 怎么选择参数n？一般设为2的幂次以充分利用矩阵运算。
* 如何选择n个训练数据？每次遍历训练数据前对所有数据进行随机排序，然后每次迭代时按顺序挑选n个训练数据直到遍历完所有的数据，这样可以避免数据的特定顺序对算法收敛带来影响。
* 如何选择学习速率α？通常采用衰减学习速率的方案：一开始算法采用较大的学习率，当误差曲线进入平台期后，减小学习速率做更精细调整。  

-------

随机梯度下降法存在的缺点：
---
1. 陷入局部最优解。
2. 鞍点。

为了解决这两个缺点，有以下几种方法:  
1. 动量方法：修改每次前进的步伐, 让前一次步伐$v_{t-1}$参与对本次步伐的更新：  
$$v_t=\gamma v_{t-1} + \eta g_t$$  
$$\theta_{t+1}=\theta_t - v_t$$  
当设$v_0=0$时，有$v_t=\frac{1-r^t}{1-r}\eta g_t$, $\gamma$一般取0.9。

2. AdaGrad: 具体到每个参数的更新上，不同参数的更新步幅是不同的，比如对于样本$x_i=(x_i^1,...,x_i^m)$ , 如果$x_i^1$很少不为0，那么对应的$w_1$更新频率也就很低。而我们希望不同参数有自适应的学习速率，也就是更新频率低的参数有较大的更新步幅，反之则步幅减小。AdaGrad采用“历史梯度平方和”来衡量不同参数的梯度的稀疏性，具体更新公式为:  
$$\theta_{t+1,j}=\theta_{t,j}-\frac{\eta}{\sqrt{\sum_{k=0}^t g_{k,j}^2 + \epsilon}}g_{t,j}$$  
分母中实现了退火过程，也就是随着时间推移，学习速率越来越小，保证算法最终收敛。  

3. RMSProp方法:AdaGrad方法问题在于，步幅是单调递减的。如果一开始没有得到一个理想的解，随着步幅减小，收敛会越来越慢。因此提出用指数加权平均的思想，对梯度按元素平方做指数加权平均，尽量关注最近几次梯度的变化值。具体来说，给定超参数$0\leq \gamma \lt 1, t=0,1...n$, 有：  
$$v_{t+1,j}=\gamma v_{t,j}+(1-\gamma)g_{t+1,j}$$  
$$\theta_{t+1,j}=\theta_{t,j}-\frac{\eta}{\sqrt{v_{t+1,j}+\epsilon}}g_{t+1,j}$$  
假设$v_{0,j}=0$,那么有$v_{t,j}=(1-\gamma)\sum_{i=1}^{t}\gamma^{t-i}g_{i,j}$, 从式子中不难看出，越接近t时刻的梯度g对$v_t$影响越大。可以将其近似为最近$1/(1-\gamma)$个时间步的梯度加权平均。所以指数加权平均解决了AdamGrad算法中学习速率越来越小的问题，可以更快逼近最优解。  

4. Adam方法：动量方法体现的是对惯性的保持，而AdaGrad和RMSProp体现的是对周围环境的感知以调整下一时刻学习速率。Adam方法同时保留了这两个特性，并将指数平均的思想同时用在这两个特性上面，具体公式：  
$$m_t=\beta_{1}m_{t-1}+(1-\beta_{1})g_t, 即m_t=(1-\beta_1)\sum_{i=1}^t\beta_1^{t-i}g_i$$  
$$v_t=\beta_{2}v_{t-1}+(1-\beta_{2})g_t^2, 即v_t=(1-\beta_2)\sum_{i=1}^t\beta_2^{t-i}g_i^2$$  
注意到移动平均的权重  
$$(1-\beta_1)\sum_{i=1}^t\beta_1^{t-i}=1-\beta_1^t\ne1$$  
同时注意到当t较小时，过去各个时间步的小批量随机梯度权值之和会较小。为了消除影响，做偏差修正，让过去各时间步小批量随机梯度权值之和为1：  
$$\hat{m_t}=\frac{m_t}{1-\beta_1^t}$$
$$\hat{v_t}=\frac{v_t}{1-\beta_2^t}$$
最终  
$$\theta_{t+1}=\theta_t-\frac{\eta}{\sqrt{\hat{v_t}+\epsilon}}\hat{m_t}$$