batch_norm主要解决的问题：  
1. 深度神经网络中，一般来说一层的输入经过某一层变换后，输出的分布和输入的分布会不一致。输出分布依赖于网络参数，而网络参数是在不断变化当中，且可能很小的改变就会导致输出的分布变动很大。因此下一层需要适应不断变化的分布，导致收敛减慢。
2. 随着训练进行，未经激活函数的网络输出X的某些维度很可能会变得很大。此时比如我们要是选了sigmoid作为激活函数，则sigmoid 的导数 $g'(x)$ 就会趋向于0。也就是进入了梯度饱和区。之前的解决方法是：选择relu，小心初始化参数并选择小的学习率。  

针对上面两个问题，在线性变化层之后，激活层之前加入batch norm层。对输入的每一维度都进行normalization的处理：
$$\hat{x}^{(k)}=\frac{x^{(k)}-E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$$  
k表示x的一个维度。这样在激活前，数据就变成了均值为0，方差为1的分布。解决了分布偏移的问题。同时很显然这样处理后可以避免某些维度上x的值太大导致进入梯度饱和区。  

但是这样处理，会丧失网络所学习出的一些信息，因此对该输出增加一次转换
$$y_i^{(k)} \leftarrow \gamma^{(k)}{\hat{x}_i^{(k)}+\beta^{(k)}}$$  
进行scale和shift，其中i表示这是某个batch中第i个样本。这里，$\gamma$和$\beta$ 是需要学习的参数。如果正好训练出来$\gamma^{(k)}=\sqrt(Var[x^{(k)}])$，$\beta^{(k)}=E[x^{(k)}]$，那么就恢复了原始的输入。